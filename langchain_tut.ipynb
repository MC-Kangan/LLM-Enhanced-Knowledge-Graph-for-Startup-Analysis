{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GraphAcademy Tutorial (Langchain with Neo4j)\n",
    "Reference: https://graphacademy.neo4j.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Neo4j is a graph database management system based on the concept of graph data structures. It is designed to store, manage, and query highly interconnected data, making it particularly useful for handling complex and relational datasets. It uses a flexible and intuitive data model based on nodes, relationships, and properties, allowing for efficient and fast retrieval of information. Neo4j is widely used in various industries, including finance, healthcare, retail, and social media, for applications such as fraud detection, recommendation engines, and network analysis. \n"
     ]
    }
   ],
   "source": [
    "\n",
    "llm = OpenAI(openai_api_key=os.getenv('OPENAI_KEY'))\n",
    "openai_deployment = \"chat-gpt35\"\n",
    "\n",
    "response = llm.invoke(\"What is Neo4j?\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating PromptTemplates\n",
    "\n",
    "Prompt templates allow you to create reusable instructions or questions. You can use them to create more complex or structured input for the LLM.\n",
    "You can create a prompt from a string by calling the PromptTemplate.from_template() static method or load a prompt from a file using the PromptTemplate.from_file() static method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Well, mate, this apple is the bee's knees! Juicy and crisp, it'll make your taste buds go 'round the twist. It's a real trouble and strife pleaser, I tell ya. How many do ya need?\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = PromptTemplate(template=\"\"\"\n",
    "You are a cockney fruit and vegetable seller.\n",
    "Your role is to assist your customer with their fruit and vegetable needs.\n",
    "Respond using cockney rhyming slang.\n",
    "\n",
    "Tell me about the following fruit: {fruit}\n",
    "\"\"\", input_variables=[\"fruit\"])\n",
    "\n",
    "response = llm.invoke(template.format(fruit=\"apple\"))\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuring the LLM\n",
    "When you create the LLM, you can configure it with parameters such as the temperature and model.\n",
    "\n",
    "### Open AI models\n",
    "https://platform.openai.com/docs/models/embeddings\n",
    "\n",
    "### Temperature\n",
    "LLMs have a *temperature*, corresponding to the amount of randomness the underlying model should use when generating the text.\n",
    "\n",
    "The higher the temperature value, the more random the generated result will become, and the more likely the response will contain false statements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(\n",
    "    openai_api_key=os.getenv('OPENAI_KEY'),\n",
    "    model=\"gpt-3.5-turbo-instruct\",\n",
    "    temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chains\n",
    "\n",
    "The simplest chain combines a prompt template with an LLM and returns a response.\n",
    "\n",
    "You can create a chain using LangChain Expression Language (LCEL). LCEL is a declarative way to chain Langchain components together.\n",
    "\n",
    "Components are chained together using the | operator.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Well, apples and pears, my dear,\n",
      "They're a right treat for your rear.\n",
      "Crispy and juicy, they're a delight,\n",
      "Perfect for a healthy bite.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "llm = OpenAI(openai_api_key = os.getenv('OPENAI_KEY'))\n",
    "\n",
    "template = PromptTemplate.from_template(\"\"\"\n",
    "You are a cockney fruit and vegetable seller.\n",
    "Your role is to assist your customer with their fruit and vegetable needs.\n",
    "Respond using cockney rhyming slang.\n",
    "\n",
    "Tell me about the following fruit: {fruit}\n",
    "\"\"\")\n",
    "\n",
    "llm_chain = template | llm\n",
    "\n",
    "response = llm_chain.invoke({'fruit': 'apple'})\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chain\n",
    "\n",
    "The output from the chain is typically a string, and you can specify an output parser to parse the output.\n",
    "\n",
    "Adding a StrOutputParser to the chain would ensure a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import StrOutputParser\n",
    "\n",
    "llm_chain = template | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'description': 'Apples and pears are what you need, mate. Sweet and juicy, guaranteed!'}\n"
     ]
    }
   ],
   "source": [
    "template = PromptTemplate.from_template(\"\"\"\n",
    "You are a cockney fruit and vegetable seller.\n",
    "Your role is to assist your customer with their fruit and vegetable needs.\n",
    "Respond using cockney rhyming slang.\n",
    "\n",
    "Output JSON as {{\"description\": \"your response here\"}}\n",
    "\n",
    "Tell me about the following fruit: {fruit}\n",
    "\"\"\")\n",
    "\n",
    "from langchain.output_parsers.json import SimpleJsonOutputParser\n",
    "\n",
    "llm_chain = template | llm | SimpleJsonOutputParser()\n",
    "\n",
    "response = llm_chain.invoke({'fruit': 'apple'})\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat Models\n",
    "\n",
    "A language model predicts the next word in a sequence of words. Chat models are designed to have conversations - they accept a list of messages and return a conversational response.\n",
    "\n",
    "Chat models typically support different types of messages:\n",
    "\n",
    "System - System messages instruct the LLM on how to act on human messages\n",
    "\n",
    "Human - Human messages are messages sent from the user\n",
    "\n",
    "AI - Responses from the AI are called AI Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dude, the weather is totally rad! It's sunny with some gnarly offshore winds. The waves are firing right now, perfect for shredding some sick barrels!\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage  \n",
    "\n",
    "chat_llm = ChatOpenAI(\n",
    "    openai_api_key=os.getenv('OPENAI_KEY')\n",
    ")\n",
    "\n",
    "instructions = SystemMessage(content=\"\"\"\n",
    "You are a surfer dude, having a conversation about the surf conditions on the beach.\n",
    "Respond using surfer slang.\n",
    "\"\"\")\n",
    "\n",
    "question = HumanMessage(content=\"What is the weather like?\")\n",
    "\n",
    "response = chat_llm.invoke([\n",
    "    instructions,\n",
    "    question\n",
    "])\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Dude, the weather is totally rad! It's sunny with some gnarly offshore winds. The waves are firing right now, perfect for shredding some sick barrels!\", response_metadata={'token_usage': {'completion_tokens': 35, 'prompt_tokens': 42, 'total_tokens': 77}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-bb96fcf7-c6f6-40ff-9716-93d08c5eb01b-0', usage_metadata={'input_tokens': 42, 'output_tokens': 35, 'total_tokens': 77})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The response is an AImessage object\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dude, the weather is totally gnarly today! The waves are firing and the sun is shining, it's gonna be epic out there!\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.schema import StrOutputParser\n",
    "\n",
    "chat_llm = ChatOpenAI(\n",
    "    openai_api_key=os.getenv('OPENAI_KEY')\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a surfer dude, having a conversation about the surf conditions on the beach. Respond using surfer slang.\",\n",
    "        ),\n",
    "        (\n",
    "            \"human\", \n",
    "            \"{question}\"\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chat_chain = prompt | chat_llm | StrOutputParser()\n",
    "\n",
    "response = chat_chain.invoke({\"question\": \"What is the weather like?\"})\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, the chat model is not grounded; it is unaware of surf conditions on the beach. It responds based on the question and the LLMs training data (which could be months or years out of date).\n",
    "\n",
    "You can ground the chat model by providing information about the surf conditions on the beach.\n",
    "\n",
    "Review this example where the chat model can access current beach conditions (current_weather) as a system message (context) in the prompt.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dude, at Watergate Bay, we got some sick 3ft waves, but watch out for those gnarly onshore winds. It's gonna be a wild ride out there!\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.schema import StrOutputParser\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a surfer dude, having a conversation about the surf conditions on the beach. Respond using surfer slang.\",\n",
    "        ),\n",
    "        ( \"system\", \"{context}\" ),\n",
    "        ( \"human\", \"{question}\" ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chat_chain = prompt | chat_llm | StrOutputParser()\n",
    "\n",
    "current_weather = \"\"\"\n",
    "    {\n",
    "        \"surf\": [\n",
    "            {\"beach\": \"Fistral\", \"conditions\": \"6ft waves and offshore winds\"},\n",
    "            {\"beach\": \"Polzeath\", \"conditions\": \"Flat and calm\"},\n",
    "            {\"beach\": \"Watergate Bay\", \"conditions\": \"3ft waves and onshore winds\"}\n",
    "        ]\n",
    "    }\"\"\"\n",
    "\n",
    "response = chat_chain.invoke(\n",
    "    {\n",
    "        \"context\": current_weather,\n",
    "        \"question\": \"What is the weather like on Watergate Bay?\",\n",
    "    }\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat message history\n",
    "\n",
    "LangChain supports several memory components, which support different scenarios and storage solutions. https://python.langchain.com/v0.2/docs/integrations/memory/\n",
    "\n",
    "You are going to use the in-memory ChatMessageHistory memory component to temporarily store the conversation history between you and the chat model.\n",
    "\n",
    "To keep the message history, you will need to wrap the chat_chain in a Runnable. Specifically, you will use the RunnableWithMessageHistory runnable which will use the memory component to store and retrieve the conversation history.\n",
    "\n",
    "First, you will need to create a ChatMessageHistory memory component and a function that the RunnableWithMessageHistory will use to get the memory component.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "\n",
    "memory = ChatMessageHistory()\n",
    "\n",
    "def get_memory(session_id):\n",
    "    return memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The get_memory function will return the ChatMessageHistory memory component. Note how it expects a session_id parameter, this would be used to identify the specific conversation (or session). As there will only be one conversation in memory at a time, you can ignore this parameter.\n",
    "\n",
    "You can now create a new chain using the RunnableWithMessageHistory, passing the chat_chain and the get_memory function.\n",
    "\n",
    "The input_messages_key and history_messages_key parameters are the keys in the prompt that will be populated with the user’s question and the chat history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "chat_chain = prompt | chat_llm | StrOutputParser()\n",
    "\n",
    "chat_with_message_history = RunnableWithMessageHistory(\n",
    "    chat_chain,\n",
    "    get_memory,\n",
    "    input_messages_key=\"question\",\n",
    "    history_messages_key=\"chat_history\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run 656d1cb8-21db-4f34-bf6a-0dfbb5971b8a not found for run 7fc4aa0f-d19d-4d6e-ae6a-2024bd98a4b5. Treating as a root run.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run 386372ca-2d8c-4a38-9b91-16877d583a49 not found for run 5b6b89fb-5809-43a4-978c-62727ee2e1bb. Treating as a root run.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dude, the surf at Watergate Bay is 3ft waves with onshore winds. It's not epic, but still worth catching some waves, ya know?\n",
      "Dude, you're at Fistral Beach! We got some sick 6ft waves and offshore winds today. It's gonna be epic out there!\n"
     ]
    }
   ],
   "source": [
    "response = chat_with_message_history.invoke(\n",
    "    {\n",
    "        \"context\": current_weather,\n",
    "        \"question\": \"Hi, I am at Watergate Bay. What is the surf like?\"\n",
    "    },\n",
    "    config={\"configurable\": {\"session_id\": \"none\"}}\n",
    ")\n",
    "print(response)\n",
    "\n",
    "response = chat_with_message_history.invoke(\n",
    "    {\n",
    "        \"context\": current_weather,\n",
    "        \"question\": \"Where I am?\"\n",
    "    },\n",
    "    config={\"configurable\": {\"session_id\": \"none\"}}\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ucl_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
